{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a5057a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "References:\n",
    "https://arxiv.org/pdf/1512.03385.pdf\n",
    "https://arxiv.org/pdf/1604.04112v4.pdf\n",
    "https://github.com/yu4u/cutout-random-erasing\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Add, Dense, Conv2D, BatchNormalization\n",
    "from tensorflow.keras.layers import Activation, AveragePooling2D, Input, Flatten\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e34ed2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 180\n",
    "n_classes = 10\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30c65c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "y_train = to_categorical(y_train, n_classes)\n",
    "y_test = to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0eb79e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet:\n",
    "    def __init__(self, size=44, stacks=3, starting_filter=16):\n",
    "        self.size = size\n",
    "        self.stacks = stacks\n",
    "        self.starting_filter = starting_filter\n",
    "        self.residual_blocks = (size - 2) // 6\n",
    "        \n",
    "    def get_model(self, input_shape=(32, 32, 3), n_classes=10):\n",
    "        n_filters = self.starting_filter\n",
    "\n",
    "        inputs = Input(shape=input_shape)\n",
    "        network = self.layer(inputs, n_filters)\n",
    "        network = self.stack(network, n_filters, True)\n",
    "\n",
    "        for _ in range(self.stacks - 1):\n",
    "            n_filters *= 2\n",
    "            network = self.stack(network, n_filters)\n",
    "\n",
    "        network = Activation('elu')(network)\n",
    "        network = AveragePooling2D(pool_size=network.shape[1])(network)\n",
    "        network = Flatten()(network)\n",
    "        outputs = Dense(n_classes, activation='softmax', \n",
    "                        kernel_initializer='he_normal')(network)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def stack(self, inputs, n_filters, first_stack=False):\n",
    "        stack = inputs\n",
    "\n",
    "        if first_stack:\n",
    "            stack = self.identity_block(stack, n_filters)\n",
    "        else:\n",
    "            stack = self.convolution_block(stack, n_filters)\n",
    "\n",
    "        for _ in range(self.residual_blocks - 1):\n",
    "            stack = self.identity_block(stack, n_filters)\n",
    "\n",
    "        return stack\n",
    "    \n",
    "    def identity_block(self, inputs, n_filters):\n",
    "        shortcut = inputs\n",
    "\n",
    "        block = self.layer(inputs, n_filters, normalize_batch=False)\n",
    "        block = self.layer(block, n_filters, activation=None)\n",
    "\n",
    "        block = Add()([shortcut, block])\n",
    "\n",
    "        return block\n",
    "\n",
    "    def convolution_block(self, inputs, n_filters, strides=2):\n",
    "        shortcut = inputs\n",
    "\n",
    "        block = self.layer(inputs, n_filters, strides=strides,\n",
    "                           normalize_batch=False)\n",
    "        block = self.layer(block, n_filters, activation=None)\n",
    "\n",
    "        shortcut = self.layer(shortcut, n_filters,\n",
    "                              kernel_size=1, strides=strides,\n",
    "                              activation=None)\n",
    "\n",
    "        block = Add()([shortcut, block])\n",
    "\n",
    "        return block\n",
    "    \n",
    "    def layer(self, inputs, n_filters, kernel_size=3,\n",
    "              strides=1, activation='elu', normalize_batch=True):\n",
    "    \n",
    "        convolution = Conv2D(n_filters, kernel_size=kernel_size,\n",
    "                             strides=strides, padding='same',\n",
    "                             kernel_initializer=\"he_normal\",\n",
    "                             kernel_regularizer=l2(1e-4))\n",
    "\n",
    "        x = convolution(inputs)\n",
    "\n",
    "        if normalize_batch:\n",
    "            x = BatchNormalization()(x)\n",
    "\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "def learning_rate_schedule(epoch):\n",
    "    new_learning_rate = learning_rate\n",
    "\n",
    "    if epoch <= 100:\n",
    "        pass\n",
    "    elif epoch > 100 and epoch <= 150:\n",
    "        new_learning_rate = learning_rate * 0.1\n",
    "    else:\n",
    "        new_learning_rate = learning_rate * 0.01\n",
    "        \n",
    "    print('Learning rate:', new_learning_rate)\n",
    "    \n",
    "    return new_learning_rate\n",
    "\n",
    "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
    "    def eraser(input_img):\n",
    "        if input_img.ndim == 3:\n",
    "            img_h, img_w, img_c = input_img.shape\n",
    "        elif input_img.ndim == 2:\n",
    "            img_h, img_w = input_img.shape\n",
    "\n",
    "        p_1 = np.random.rand()\n",
    "\n",
    "        if p_1 > p:\n",
    "            return input_img\n",
    "\n",
    "        while True:\n",
    "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
    "            r = np.random.uniform(r_1, r_2)\n",
    "            w = int(np.sqrt(s / r))\n",
    "            h = int(np.sqrt(s * r))\n",
    "            left = np.random.randint(0, img_w)\n",
    "            top = np.random.randint(0, img_h)\n",
    "\n",
    "            if left + w <= img_w and top + h <= img_h:\n",
    "                break\n",
    "\n",
    "        if pixel_level:\n",
    "            if input_img.ndim == 3:\n",
    "                c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
    "            if input_img.ndim == 2:\n",
    "                c = np.random.uniform(v_l, v_h, (h, w))\n",
    "        else:\n",
    "            c = np.random.uniform(v_l, v_h)\n",
    "\n",
    "        input_img[top:top + h, left:left + w] = c\n",
    "\n",
    "        return input_img\n",
    "\n",
    "    return eraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3de5a3fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_135 (Conv2D)             (None, 32, 32, 16)   448         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 32, 32, 16)   64          conv2d_135[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 32, 32, 16)   0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_136 (Conv2D)             (None, 32, 32, 16)   2320        activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 32, 32, 16)   0           conv2d_136[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_137 (Conv2D)             (None, 32, 32, 16)   2320        activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 32, 32, 16)   64          conv2d_137[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_63 (Add)                    (None, 32, 32, 16)   0           activation_69[0][0]              \n",
      "                                                                 batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_138 (Conv2D)             (None, 32, 32, 16)   2320        add_63[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 32, 32, 16)   0           conv2d_138[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_139 (Conv2D)             (None, 32, 32, 16)   2320        activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 32, 32, 16)   64          conv2d_139[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_64 (Add)                    (None, 32, 32, 16)   0           add_63[0][0]                     \n",
      "                                                                 batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_140 (Conv2D)             (None, 32, 32, 16)   2320        add_64[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 32, 32, 16)   0           conv2d_140[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_141 (Conv2D)             (None, 32, 32, 16)   2320        activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 32, 32, 16)   64          conv2d_141[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_65 (Add)                    (None, 32, 32, 16)   0           add_64[0][0]                     \n",
      "                                                                 batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_142 (Conv2D)             (None, 32, 32, 16)   2320        add_65[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 32, 32, 16)   0           conv2d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_143 (Conv2D)             (None, 32, 32, 16)   2320        activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 32, 32, 16)   64          conv2d_143[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_66 (Add)                    (None, 32, 32, 16)   0           add_65[0][0]                     \n",
      "                                                                 batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_144 (Conv2D)             (None, 32, 32, 16)   2320        add_66[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 32, 32, 16)   0           conv2d_144[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_145 (Conv2D)             (None, 32, 32, 16)   2320        activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 32, 32, 16)   64          conv2d_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_67 (Add)                    (None, 32, 32, 16)   0           add_66[0][0]                     \n",
      "                                                                 batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_146 (Conv2D)             (None, 32, 32, 16)   2320        add_67[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 32, 32, 16)   0           conv2d_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_147 (Conv2D)             (None, 32, 32, 16)   2320        activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 32, 32, 16)   64          conv2d_147[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_68 (Add)                    (None, 32, 32, 16)   0           add_67[0][0]                     \n",
      "                                                                 batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_148 (Conv2D)             (None, 32, 32, 16)   2320        add_68[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 32, 32, 16)   0           conv2d_148[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_149 (Conv2D)             (None, 32, 32, 16)   2320        activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 32, 32, 16)   64          conv2d_149[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_69 (Add)                    (None, 32, 32, 16)   0           add_68[0][0]                     \n",
      "                                                                 batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_150 (Conv2D)             (None, 16, 16, 32)   4640        add_69[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 16, 16, 32)   0           conv2d_150[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_152 (Conv2D)             (None, 16, 16, 32)   544         add_69[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_151 (Conv2D)             (None, 16, 16, 32)   9248        activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 16, 16, 32)   128         conv2d_152[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 16, 16, 32)   128         conv2d_151[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_70 (Add)                    (None, 16, 16, 32)   0           batch_normalization_81[0][0]     \n",
      "                                                                 batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_153 (Conv2D)             (None, 16, 16, 32)   9248        add_70[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 16, 16, 32)   0           conv2d_153[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_154 (Conv2D)             (None, 16, 16, 32)   9248        activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 16, 16, 32)   128         conv2d_154[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_71 (Add)                    (None, 16, 16, 32)   0           add_70[0][0]                     \n",
      "                                                                 batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_155 (Conv2D)             (None, 16, 16, 32)   9248        add_71[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 16, 16, 32)   0           conv2d_155[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_156 (Conv2D)             (None, 16, 16, 32)   9248        activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 16, 16, 32)   128         conv2d_156[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_72 (Add)                    (None, 16, 16, 32)   0           add_71[0][0]                     \n",
      "                                                                 batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_157 (Conv2D)             (None, 16, 16, 32)   9248        add_72[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 16, 16, 32)   0           conv2d_157[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_158 (Conv2D)             (None, 16, 16, 32)   9248        activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 16, 16, 32)   128         conv2d_158[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_73 (Add)                    (None, 16, 16, 32)   0           add_72[0][0]                     \n",
      "                                                                 batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_159 (Conv2D)             (None, 16, 16, 32)   9248        add_73[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 16, 16, 32)   0           conv2d_159[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_160 (Conv2D)             (None, 16, 16, 32)   9248        activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 16, 16, 32)   128         conv2d_160[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_74 (Add)                    (None, 16, 16, 32)   0           add_73[0][0]                     \n",
      "                                                                 batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_161 (Conv2D)             (None, 16, 16, 32)   9248        add_74[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 16, 16, 32)   0           conv2d_161[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_162 (Conv2D)             (None, 16, 16, 32)   9248        activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 16, 16, 32)   128         conv2d_162[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_75 (Add)                    (None, 16, 16, 32)   0           add_74[0][0]                     \n",
      "                                                                 batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_163 (Conv2D)             (None, 16, 16, 32)   9248        add_75[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 16, 16, 32)   0           conv2d_163[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_164 (Conv2D)             (None, 16, 16, 32)   9248        activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 16, 16, 32)   128         conv2d_164[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_76 (Add)                    (None, 16, 16, 32)   0           add_75[0][0]                     \n",
      "                                                                 batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_165 (Conv2D)             (None, 8, 8, 64)     18496       add_76[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 8, 8, 64)     0           conv2d_165[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_167 (Conv2D)             (None, 8, 8, 64)     2112        add_76[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_166 (Conv2D)             (None, 8, 8, 64)     36928       activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 8, 8, 64)     256         conv2d_167[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 8, 8, 64)     256         conv2d_166[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_77 (Add)                    (None, 8, 8, 64)     0           batch_normalization_89[0][0]     \n",
      "                                                                 batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_168 (Conv2D)             (None, 8, 8, 64)     36928       add_77[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 8, 8, 64)     0           conv2d_168[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_169 (Conv2D)             (None, 8, 8, 64)     36928       activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 8, 8, 64)     256         conv2d_169[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_78 (Add)                    (None, 8, 8, 64)     0           add_77[0][0]                     \n",
      "                                                                 batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_170 (Conv2D)             (None, 8, 8, 64)     36928       add_78[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 8, 8, 64)     0           conv2d_170[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_171 (Conv2D)             (None, 8, 8, 64)     36928       activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 8, 8, 64)     256         conv2d_171[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_79 (Add)                    (None, 8, 8, 64)     0           add_78[0][0]                     \n",
      "                                                                 batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_172 (Conv2D)             (None, 8, 8, 64)     36928       add_79[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 8, 8, 64)     0           conv2d_172[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_173 (Conv2D)             (None, 8, 8, 64)     36928       activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 8, 8, 64)     256         conv2d_173[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_80 (Add)                    (None, 8, 8, 64)     0           add_79[0][0]                     \n",
      "                                                                 batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_174 (Conv2D)             (None, 8, 8, 64)     36928       add_80[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 8, 8, 64)     0           conv2d_174[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_175 (Conv2D)             (None, 8, 8, 64)     36928       activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 8, 8, 64)     256         conv2d_175[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_81 (Add)                    (None, 8, 8, 64)     0           add_80[0][0]                     \n",
      "                                                                 batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_176 (Conv2D)             (None, 8, 8, 64)     36928       add_81[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 8, 8, 64)     0           conv2d_176[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_177 (Conv2D)             (None, 8, 8, 64)     36928       activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 8, 8, 64)     256         conv2d_177[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_82 (Add)                    (None, 8, 8, 64)     0           add_81[0][0]                     \n",
      "                                                                 batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_178 (Conv2D)             (None, 8, 8, 64)     36928       add_82[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 8, 8, 64)     0           conv2d_178[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_179 (Conv2D)             (None, 8, 8, 64)     36928       activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 8, 8, 64)     256         conv2d_179[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_83 (Add)                    (None, 8, 8, 64)     0           add_82[0][0]                     \n",
      "                                                                 batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 8, 8, 64)     0           add_83[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 1, 1, 64)     0           activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 64)           0           average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 10)           650         flatten_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 663,242\n",
      "Trainable params: 661,450\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "resnet = Resnet()\n",
    "\n",
    "model = resnet.get_model()\n",
    "\n",
    "optimizer = SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22558699",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 30s 67ms/step - loss: 1.9564 - accuracy: 0.4099 - val_loss: 1.9104 - val_accuracy: 0.4790\n",
      "Epoch 2/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 1.5425 - accuracy: 0.5566 - val_loss: 1.6927 - val_accuracy: 0.5715\n",
      "Epoch 3/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 1.3299 - accuracy: 0.6321 - val_loss: 1.1486 - val_accuracy: 0.7031\n",
      "Epoch 4/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 1.1810 - accuracy: 0.6832 - val_loss: 1.0400 - val_accuracy: 0.7375\n",
      "Epoch 5/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 1.0844 - accuracy: 0.7140 - val_loss: 1.0425 - val_accuracy: 0.7334\n",
      "Epoch 6/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 1.0205 - accuracy: 0.7331 - val_loss: 1.2108 - val_accuracy: 0.6837\n",
      "Epoch 7/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 62ms/step - loss: 0.9728 - accuracy: 0.7462 - val_loss: 0.9566 - val_accuracy: 0.7586\n",
      "Epoch 8/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.9223 - accuracy: 0.7648 - val_loss: 0.9185 - val_accuracy: 0.7707\n",
      "Epoch 9/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.8931 - accuracy: 0.7716 - val_loss: 0.8003 - val_accuracy: 0.8111\n",
      "Epoch 10/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 62ms/step - loss: 0.8588 - accuracy: 0.7818 - val_loss: 0.8712 - val_accuracy: 0.7930\n",
      "Epoch 11/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 62ms/step - loss: 0.8364 - accuracy: 0.7904 - val_loss: 0.7870 - val_accuracy: 0.8056\n",
      "Epoch 12/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 0.8179 - accuracy: 0.7951 - val_loss: 0.6994 - val_accuracy: 0.8405\n",
      "Epoch 13/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 62ms/step - loss: 0.7989 - accuracy: 0.8017 - val_loss: 0.7394 - val_accuracy: 0.8244\n",
      "Epoch 14/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.7794 - accuracy: 0.8091 - val_loss: 0.7068 - val_accuracy: 0.8373\n",
      "Epoch 15/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.7721 - accuracy: 0.8090 - val_loss: 0.7939 - val_accuracy: 0.8107\n",
      "Epoch 16/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 0.7621 - accuracy: 0.8149 - val_loss: 0.7381 - val_accuracy: 0.8294\n",
      "Epoch 17/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.7490 - accuracy: 0.8189 - val_loss: 0.7314 - val_accuracy: 0.8282\n",
      "Epoch 18/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 0.7405 - accuracy: 0.8230 - val_loss: 0.7820 - val_accuracy: 0.8203\n",
      "Epoch 19/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.7300 - accuracy: 0.8272 - val_loss: 0.7011 - val_accuracy: 0.8379\n",
      "Epoch 20/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 62ms/step - loss: 0.7249 - accuracy: 0.8288 - val_loss: 0.8853 - val_accuracy: 0.7710\n",
      "Epoch 21/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.7170 - accuracy: 0.8322 - val_loss: 0.6466 - val_accuracy: 0.8615\n",
      "Epoch 22/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.7127 - accuracy: 0.8330 - val_loss: 0.6937 - val_accuracy: 0.8493\n",
      "Epoch 23/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 62ms/step - loss: 0.7113 - accuracy: 0.8348 - val_loss: 0.9956 - val_accuracy: 0.7386\n",
      "Epoch 24/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.7040 - accuracy: 0.8386 - val_loss: 0.7201 - val_accuracy: 0.8422\n",
      "Epoch 25/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.7033 - accuracy: 0.8389 - val_loss: 0.6674 - val_accuracy: 0.8559\n",
      "Epoch 26/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 62ms/step - loss: 0.6851 - accuracy: 0.8455 - val_loss: 0.7347 - val_accuracy: 0.8418\n",
      "Epoch 27/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 0.6881 - accuracy: 0.8459 - val_loss: 0.6324 - val_accuracy: 0.8664\n",
      "Epoch 28/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 62ms/step - loss: 0.6831 - accuracy: 0.8465 - val_loss: 0.6579 - val_accuracy: 0.8600\n",
      "Epoch 29/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 62ms/step - loss: 0.6740 - accuracy: 0.8512 - val_loss: 0.6726 - val_accuracy: 0.8586\n",
      "Epoch 30/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 62ms/step - loss: 0.6780 - accuracy: 0.8493 - val_loss: 0.7520 - val_accuracy: 0.8341\n",
      "Epoch 31/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 62ms/step - loss: 0.6726 - accuracy: 0.8522 - val_loss: 0.6568 - val_accuracy: 0.8655\n",
      "Epoch 32/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 60ms/step - loss: 0.6803 - accuracy: 0.8501 - val_loss: 0.7470 - val_accuracy: 0.8390\n",
      "Epoch 33/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 0.6704 - accuracy: 0.8554 - val_loss: 0.6838 - val_accuracy: 0.8599\n",
      "Epoch 34/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 62ms/step - loss: 0.6654 - accuracy: 0.8564 - val_loss: 0.7144 - val_accuracy: 0.8527\n",
      "Epoch 35/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 0.6655 - accuracy: 0.8574 - val_loss: 0.7034 - val_accuracy: 0.8525\n",
      "Epoch 36/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 62ms/step - loss: 0.6670 - accuracy: 0.8576 - val_loss: 0.6359 - val_accuracy: 0.8741\n",
      "Epoch 37/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 0.6602 - accuracy: 0.8591 - val_loss: 0.7371 - val_accuracy: 0.8432\n",
      "Epoch 38/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 0.6589 - accuracy: 0.8598 - val_loss: 0.6792 - val_accuracy: 0.8606\n",
      "Epoch 39/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.6589 - accuracy: 0.8594 - val_loss: 0.6644 - val_accuracy: 0.8659\n",
      "Epoch 40/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.6597 - accuracy: 0.8610 - val_loss: 0.6330 - val_accuracy: 0.8774\n",
      "Epoch 41/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.6589 - accuracy: 0.8620 - val_loss: 0.6729 - val_accuracy: 0.8617\n",
      "Epoch 42/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.6515 - accuracy: 0.8652 - val_loss: 0.7011 - val_accuracy: 0.8600\n",
      "Epoch 43/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 62ms/step - loss: 0.6547 - accuracy: 0.8628 - val_loss: 0.7941 - val_accuracy: 0.8278\n",
      "Epoch 44/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.6488 - accuracy: 0.8655 - val_loss: 0.6125 - val_accuracy: 0.8823\n",
      "Epoch 45/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.6509 - accuracy: 0.8658 - val_loss: 0.7719 - val_accuracy: 0.8353\n",
      "Epoch 46/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 62ms/step - loss: 0.6539 - accuracy: 0.8653 - val_loss: 0.6758 - val_accuracy: 0.8654\n",
      "Epoch 47/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 62ms/step - loss: 0.6498 - accuracy: 0.8674 - val_loss: 0.6338 - val_accuracy: 0.8757\n",
      "Epoch 48/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 62ms/step - loss: 0.6413 - accuracy: 0.8700 - val_loss: 0.6555 - val_accuracy: 0.8709\n",
      "Epoch 49/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 62ms/step - loss: 0.6484 - accuracy: 0.8657 - val_loss: 0.6652 - val_accuracy: 0.8666\n",
      "Epoch 50/180\n",
      "Learning rate: 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 25s 63ms/step - loss: 0.6366 - accuracy: 0.8710 - val_loss: 0.6601 - val_accuracy: 0.8731\n",
      "Epoch 51/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 62ms/step - loss: 0.6467 - accuracy: 0.8684 - val_loss: 0.6894 - val_accuracy: 0.8594\n",
      "Epoch 52/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.6483 - accuracy: 0.8676 - val_loss: 0.7589 - val_accuracy: 0.8469\n",
      "Epoch 53/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.6440 - accuracy: 0.8704 - val_loss: 0.6871 - val_accuracy: 0.8641\n",
      "Epoch 54/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.6443 - accuracy: 0.8710 - val_loss: 0.6902 - val_accuracy: 0.8635\n",
      "Epoch 55/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.6511 - accuracy: 0.8683 - val_loss: 0.6677 - val_accuracy: 0.8681\n",
      "Epoch 56/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.6388 - accuracy: 0.8727 - val_loss: 0.7020 - val_accuracy: 0.8556\n",
      "Epoch 57/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.6419 - accuracy: 0.8718 - val_loss: 0.6433 - val_accuracy: 0.8772\n",
      "Epoch 58/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 62ms/step - loss: 0.6395 - accuracy: 0.8728 - val_loss: 0.7091 - val_accuracy: 0.8594\n",
      "Epoch 59/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 62ms/step - loss: 0.6394 - accuracy: 0.8716 - val_loss: 0.6025 - val_accuracy: 0.8906\n",
      "Epoch 60/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.6305 - accuracy: 0.8754 - val_loss: 0.6579 - val_accuracy: 0.8730\n",
      "Epoch 61/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.6442 - accuracy: 0.8716 - val_loss: 0.6570 - val_accuracy: 0.8778\n",
      "Epoch 62/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.6377 - accuracy: 0.8743 - val_loss: 0.6917 - val_accuracy: 0.8645\n",
      "Epoch 63/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.6374 - accuracy: 0.8736 - val_loss: 0.6941 - val_accuracy: 0.8575\n",
      "Epoch 64/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 0.6314 - accuracy: 0.8760 - val_loss: 0.7043 - val_accuracy: 0.8595\n",
      "Epoch 65/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 62ms/step - loss: 0.6326 - accuracy: 0.8752 - val_loss: 0.6809 - val_accuracy: 0.8705\n",
      "Epoch 66/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 0.6390 - accuracy: 0.8735 - val_loss: 0.7408 - val_accuracy: 0.8560\n",
      "Epoch 67/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 0.6300 - accuracy: 0.8774 - val_loss: 0.6608 - val_accuracy: 0.8725\n",
      "Epoch 68/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 0.6327 - accuracy: 0.8772 - val_loss: 0.7315 - val_accuracy: 0.8602\n",
      "Epoch 69/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 0.6265 - accuracy: 0.8775 - val_loss: 0.6411 - val_accuracy: 0.8775\n",
      "Epoch 70/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 0.6313 - accuracy: 0.8768 - val_loss: 0.6660 - val_accuracy: 0.8710\n",
      "Epoch 71/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 0.6330 - accuracy: 0.8768 - val_loss: 1.7476 - val_accuracy: 0.5134\n",
      "Epoch 72/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 0.6269 - accuracy: 0.8774 - val_loss: 0.6624 - val_accuracy: 0.8783\n",
      "Epoch 73/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 60ms/step - loss: 0.6257 - accuracy: 0.8782 - val_loss: 0.6270 - val_accuracy: 0.8890\n",
      "Epoch 74/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 0.6312 - accuracy: 0.8776 - val_loss: 0.7293 - val_accuracy: 0.8564\n",
      "Epoch 75/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 60ms/step - loss: 0.6264 - accuracy: 0.8797 - val_loss: 0.6292 - val_accuracy: 0.8863\n",
      "Epoch 76/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 0.6309 - accuracy: 0.8783 - val_loss: 0.6699 - val_accuracy: 0.8703\n",
      "Epoch 77/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 62ms/step - loss: 0.6280 - accuracy: 0.8781 - val_loss: 0.5929 - val_accuracy: 0.8959\n",
      "Epoch 78/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 62ms/step - loss: 0.6234 - accuracy: 0.8797 - val_loss: 0.6531 - val_accuracy: 0.8829\n",
      "Epoch 79/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 60ms/step - loss: 0.6330 - accuracy: 0.8785 - val_loss: 0.6724 - val_accuracy: 0.8742\n",
      "Epoch 80/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 62ms/step - loss: 0.6258 - accuracy: 0.8797 - val_loss: 0.7479 - val_accuracy: 0.8471\n",
      "Epoch 81/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 60ms/step - loss: 0.6275 - accuracy: 0.8776 - val_loss: 1.0147 - val_accuracy: 0.7595\n",
      "Epoch 82/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 62ms/step - loss: 0.6223 - accuracy: 0.8800 - val_loss: 0.7036 - val_accuracy: 0.8627\n",
      "Epoch 83/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 62ms/step - loss: 0.6269 - accuracy: 0.8793 - val_loss: 0.6493 - val_accuracy: 0.8764\n",
      "Epoch 84/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 60ms/step - loss: 0.6243 - accuracy: 0.8811 - val_loss: 0.6365 - val_accuracy: 0.8861\n",
      "Epoch 85/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 62ms/step - loss: 0.6228 - accuracy: 0.8804 - val_loss: 0.6042 - val_accuracy: 0.8928\n",
      "Epoch 86/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 62ms/step - loss: 0.6295 - accuracy: 0.8795 - val_loss: 0.5819 - val_accuracy: 0.8976\n",
      "Epoch 87/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 60ms/step - loss: 0.6226 - accuracy: 0.8807 - val_loss: 0.7999 - val_accuracy: 0.8341\n",
      "Epoch 88/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 62ms/step - loss: 0.6211 - accuracy: 0.8817 - val_loss: 0.6312 - val_accuracy: 0.8828\n",
      "Epoch 89/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.6139 - accuracy: 0.8830 - val_loss: 0.6413 - val_accuracy: 0.8786\n",
      "Epoch 90/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.6201 - accuracy: 0.8836 - val_loss: 0.6027 - val_accuracy: 0.8924\n",
      "Epoch 91/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.6204 - accuracy: 0.8821 - val_loss: 0.6058 - val_accuracy: 0.8888\n",
      "Epoch 92/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 62ms/step - loss: 0.6249 - accuracy: 0.8800 - val_loss: 0.6481 - val_accuracy: 0.8785\n",
      "Epoch 93/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 62ms/step - loss: 0.6234 - accuracy: 0.8818 - val_loss: 0.6924 - val_accuracy: 0.8717\n",
      "Epoch 94/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 62ms/step - loss: 0.6273 - accuracy: 0.8809 - val_loss: 0.6188 - val_accuracy: 0.8897\n",
      "Epoch 95/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 62ms/step - loss: 0.6163 - accuracy: 0.8840 - val_loss: 0.6114 - val_accuracy: 0.8909\n",
      "Epoch 96/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 62ms/step - loss: 0.6210 - accuracy: 0.8832 - val_loss: 0.6279 - val_accuracy: 0.8835\n",
      "Epoch 97/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.6188 - accuracy: 0.8833 - val_loss: 0.6164 - val_accuracy: 0.8935\n",
      "Epoch 98/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.6234 - accuracy: 0.8821 - val_loss: 0.6277 - val_accuracy: 0.8857\n",
      "Epoch 99/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 62ms/step - loss: 0.6147 - accuracy: 0.8852 - val_loss: 0.7912 - val_accuracy: 0.8323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 62ms/step - loss: 0.6166 - accuracy: 0.8845 - val_loss: 0.9341 - val_accuracy: 0.7996\n",
      "Epoch 101/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 62ms/step - loss: 0.6229 - accuracy: 0.8817 - val_loss: 0.7159 - val_accuracy: 0.8614\n",
      "Epoch 102/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.5351 - accuracy: 0.9128 - val_loss: 0.4985 - val_accuracy: 0.9264\n",
      "Epoch 103/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.4785 - accuracy: 0.9307 - val_loss: 0.4798 - val_accuracy: 0.9316\n",
      "Epoch 104/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 65ms/step - loss: 0.4589 - accuracy: 0.9376 - val_loss: 0.4685 - val_accuracy: 0.9337\n",
      "Epoch 105/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.4456 - accuracy: 0.9404 - val_loss: 0.4591 - val_accuracy: 0.9374\n",
      "Epoch 106/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 25s 62ms/step - loss: 0.4326 - accuracy: 0.9442 - val_loss: 0.4538 - val_accuracy: 0.9365\n",
      "Epoch 107/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.4235 - accuracy: 0.9454 - val_loss: 0.4505 - val_accuracy: 0.9380\n",
      "Epoch 108/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 0.4166 - accuracy: 0.9473 - val_loss: 0.4402 - val_accuracy: 0.9397\n",
      "Epoch 109/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.4114 - accuracy: 0.9460 - val_loss: 0.4418 - val_accuracy: 0.9384\n",
      "Epoch 110/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 24s 62ms/step - loss: 0.4011 - accuracy: 0.9492 - val_loss: 0.4370 - val_accuracy: 0.9388\n",
      "Epoch 111/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 25s 62ms/step - loss: 0.3913 - accuracy: 0.9510 - val_loss: 0.4332 - val_accuracy: 0.9391\n",
      "Epoch 112/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.3830 - accuracy: 0.9530 - val_loss: 0.4278 - val_accuracy: 0.9406\n",
      "Epoch 113/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.3798 - accuracy: 0.9520 - val_loss: 0.4229 - val_accuracy: 0.9408\n",
      "Epoch 114/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.3740 - accuracy: 0.9538 - val_loss: 0.4225 - val_accuracy: 0.9411\n",
      "Epoch 115/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.3652 - accuracy: 0.9554 - val_loss: 0.4163 - val_accuracy: 0.9440\n",
      "Epoch 116/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 25s 65ms/step - loss: 0.3619 - accuracy: 0.9549 - val_loss: 0.4132 - val_accuracy: 0.9417\n",
      "Epoch 117/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.3585 - accuracy: 0.9556 - val_loss: 0.4122 - val_accuracy: 0.9407\n",
      "Epoch 118/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 24s 62ms/step - loss: 0.3487 - accuracy: 0.9575 - val_loss: 0.4147 - val_accuracy: 0.9406\n",
      "Epoch 119/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 24s 62ms/step - loss: 0.3499 - accuracy: 0.9565 - val_loss: 0.4012 - val_accuracy: 0.9429\n",
      "Epoch 120/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 24s 62ms/step - loss: 0.3430 - accuracy: 0.9573 - val_loss: 0.4006 - val_accuracy: 0.9425\n",
      "Epoch 121/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.3372 - accuracy: 0.9582 - val_loss: 0.4002 - val_accuracy: 0.9459\n",
      "Epoch 122/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.3349 - accuracy: 0.9588 - val_loss: 0.3983 - val_accuracy: 0.9420\n",
      "Epoch 123/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.3266 - accuracy: 0.9612 - val_loss: 0.3984 - val_accuracy: 0.9423\n",
      "Epoch 124/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.3267 - accuracy: 0.9591 - val_loss: 0.3922 - val_accuracy: 0.9438\n",
      "Epoch 125/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 65ms/step - loss: 0.3235 - accuracy: 0.9598 - val_loss: 0.3990 - val_accuracy: 0.9397\n",
      "Epoch 126/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 65ms/step - loss: 0.3177 - accuracy: 0.9610 - val_loss: 0.3853 - val_accuracy: 0.9426\n",
      "Epoch 127/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.3138 - accuracy: 0.9606 - val_loss: 0.3860 - val_accuracy: 0.9440\n",
      "Epoch 128/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.3079 - accuracy: 0.9612 - val_loss: 0.3834 - val_accuracy: 0.9442\n",
      "Epoch 129/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.3079 - accuracy: 0.9606 - val_loss: 0.3842 - val_accuracy: 0.9420\n",
      "Epoch 130/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 65ms/step - loss: 0.3039 - accuracy: 0.9612 - val_loss: 0.3861 - val_accuracy: 0.9416\n",
      "Epoch 131/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 65ms/step - loss: 0.3042 - accuracy: 0.9608 - val_loss: 0.3828 - val_accuracy: 0.9406\n",
      "Epoch 132/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.2975 - accuracy: 0.9628 - val_loss: 0.3898 - val_accuracy: 0.9406\n",
      "Epoch 133/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.2938 - accuracy: 0.9635 - val_loss: 0.3835 - val_accuracy: 0.9416\n",
      "Epoch 134/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 65ms/step - loss: 0.2923 - accuracy: 0.9624 - val_loss: 0.3864 - val_accuracy: 0.9414\n",
      "Epoch 135/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.2846 - accuracy: 0.9646 - val_loss: 0.3736 - val_accuracy: 0.9450\n",
      "Epoch 136/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 65ms/step - loss: 0.2852 - accuracy: 0.9631 - val_loss: 0.3751 - val_accuracy: 0.9421\n",
      "Epoch 137/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.2825 - accuracy: 0.9633 - val_loss: 0.3796 - val_accuracy: 0.9398\n",
      "Epoch 138/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.2824 - accuracy: 0.9623 - val_loss: 0.3770 - val_accuracy: 0.9410\n",
      "Epoch 139/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.2800 - accuracy: 0.9633 - val_loss: 0.3677 - val_accuracy: 0.9431\n",
      "Epoch 140/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.2746 - accuracy: 0.9641 - val_loss: 0.3660 - val_accuracy: 0.9436\n",
      "Epoch 141/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.2728 - accuracy: 0.9632 - val_loss: 0.3663 - val_accuracy: 0.9423\n",
      "Epoch 142/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 65ms/step - loss: 0.2686 - accuracy: 0.9645 - val_loss: 0.3580 - val_accuracy: 0.9441\n",
      "Epoch 143/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 65ms/step - loss: 0.2656 - accuracy: 0.9656 - val_loss: 0.3689 - val_accuracy: 0.9417\n",
      "Epoch 144/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.2634 - accuracy: 0.9660 - val_loss: 0.3647 - val_accuracy: 0.9417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 65ms/step - loss: 0.2633 - accuracy: 0.9651 - val_loss: 0.3616 - val_accuracy: 0.9409\n",
      "Epoch 146/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 65ms/step - loss: 0.2620 - accuracy: 0.9646 - val_loss: 0.3651 - val_accuracy: 0.9430\n",
      "Epoch 147/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.2621 - accuracy: 0.9633 - val_loss: 0.3671 - val_accuracy: 0.9399\n",
      "Epoch 148/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.2601 - accuracy: 0.9634 - val_loss: 0.3612 - val_accuracy: 0.9385\n",
      "Epoch 149/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.2548 - accuracy: 0.9659 - val_loss: 0.3645 - val_accuracy: 0.9396\n",
      "Epoch 150/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.2534 - accuracy: 0.9657 - val_loss: 0.3490 - val_accuracy: 0.9417\n",
      "Epoch 151/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.2512 - accuracy: 0.9652 - val_loss: 0.3621 - val_accuracy: 0.9400\n",
      "Epoch 152/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.2379 - accuracy: 0.9704 - val_loss: 0.3356 - val_accuracy: 0.9475\n",
      "Epoch 153/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 25s 62ms/step - loss: 0.2334 - accuracy: 0.9720 - val_loss: 0.3340 - val_accuracy: 0.9477\n",
      "Epoch 154/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.2287 - accuracy: 0.9736 - val_loss: 0.3348 - val_accuracy: 0.9471\n",
      "Epoch 155/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.2275 - accuracy: 0.9741 - val_loss: 0.3334 - val_accuracy: 0.9492\n",
      "Epoch 156/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.2272 - accuracy: 0.9740 - val_loss: 0.3340 - val_accuracy: 0.9475\n",
      "Epoch 157/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.2231 - accuracy: 0.9752 - val_loss: 0.3326 - val_accuracy: 0.9486\n",
      "Epoch 158/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 27s 67ms/step - loss: 0.2237 - accuracy: 0.9750 - val_loss: 0.3317 - val_accuracy: 0.9484\n",
      "Epoch 159/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.2210 - accuracy: 0.9761 - val_loss: 0.3299 - val_accuracy: 0.9485\n",
      "Epoch 160/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.2213 - accuracy: 0.9757 - val_loss: 0.3296 - val_accuracy: 0.9487\n",
      "Epoch 161/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 27s 67ms/step - loss: 0.2205 - accuracy: 0.9754 - val_loss: 0.3291 - val_accuracy: 0.9488\n",
      "Epoch 162/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.2187 - accuracy: 0.9766 - val_loss: 0.3301 - val_accuracy: 0.9492\n",
      "Epoch 163/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.2204 - accuracy: 0.9763 - val_loss: 0.3286 - val_accuracy: 0.9484\n",
      "Epoch 164/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.2216 - accuracy: 0.9753 - val_loss: 0.3291 - val_accuracy: 0.9478\n",
      "Epoch 165/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.2191 - accuracy: 0.9763 - val_loss: 0.3284 - val_accuracy: 0.9486\n",
      "Epoch 166/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.2182 - accuracy: 0.9761 - val_loss: 0.3295 - val_accuracy: 0.9487\n",
      "Epoch 167/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.2158 - accuracy: 0.9773 - val_loss: 0.3300 - val_accuracy: 0.9479\n",
      "Epoch 168/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.2170 - accuracy: 0.9758 - val_loss: 0.3298 - val_accuracy: 0.9488\n",
      "Epoch 169/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.2172 - accuracy: 0.9755 - val_loss: 0.3295 - val_accuracy: 0.9479\n",
      "Epoch 170/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.2132 - accuracy: 0.9778 - val_loss: 0.3288 - val_accuracy: 0.9489\n",
      "Epoch 171/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.2147 - accuracy: 0.9772 - val_loss: 0.3299 - val_accuracy: 0.9491\n",
      "Epoch 172/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.2160 - accuracy: 0.9765 - val_loss: 0.3304 - val_accuracy: 0.9486\n",
      "Epoch 173/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.2165 - accuracy: 0.9765 - val_loss: 0.3272 - val_accuracy: 0.9495\n",
      "Epoch 174/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.2153 - accuracy: 0.9765 - val_loss: 0.3280 - val_accuracy: 0.9497\n",
      "Epoch 175/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.2153 - accuracy: 0.9766 - val_loss: 0.3274 - val_accuracy: 0.9489\n",
      "Epoch 176/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.2123 - accuracy: 0.9782 - val_loss: 0.3279 - val_accuracy: 0.9483\n",
      "Epoch 177/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.2137 - accuracy: 0.9766 - val_loss: 0.3292 - val_accuracy: 0.9488\n",
      "Epoch 178/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 25s 62ms/step - loss: 0.2126 - accuracy: 0.9773 - val_loss: 0.3289 - val_accuracy: 0.9485\n",
      "Epoch 179/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.2132 - accuracy: 0.9780 - val_loss: 0.3275 - val_accuracy: 0.9495\n",
      "Epoch 180/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 25s 65ms/step - loss: 0.2127 - accuracy: 0.9768 - val_loss: 0.3265 - val_accuracy: 0.9497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\acnn\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    }
   ],
   "source": [
    "lr_scheduler = LearningRateScheduler(learning_rate_schedule)\n",
    "callbacks = [lr_scheduler]\n",
    "\n",
    "datagen = ImageDataGenerator(width_shift_range=4,\n",
    "                             height_shift_range=4,\n",
    "                             horizontal_flip=True,\n",
    "                             preprocessing_function=get_random_eraser(p=1, pixel_level=True))\n",
    "datagen.fit(x_train)\n",
    "\n",
    "model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "          validation_data=(x_test, y_test),\n",
    "          epochs=epochs, workers=4,\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aa6929",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
